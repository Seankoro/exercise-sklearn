{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Pipelines Exercise\n",
    "*Made by viga@itu.dk and thso@itu.dk*\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this exercise you'll be working with the [Wine Quality Dataset](https://archive.ics.uci.edu/ml/datasets/wine+quality) from the UCI Machine Learning Repository. The dataset consists of 11 features and a quality score for 4898 white wine samples and 1599 red wine samples. The goal is to predict the quality of the wine based on the features.\n",
    "\n",
    "The datasets are located in the `data` folder. The `winequality-red.csv` file contains the red wine samples and the `winequality-white.csv` file contains the white wine samples. Lastly, the `winequality.names` file contains a description of the dataset.\n",
    "\n",
    "The goal of this exercise is to get you familiar with the [Scikit-learn Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) API. You'll be using pipelines to perform feature scaling and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data\n",
    "\n",
    "You can either load the red-wine dataset or the white-wine dataset. You can also load both datasets and combine them if you want.\n",
    "\n",
    "Both datasets are available in the `data` folder, and are called `winequality-red.csv` and `winequality-white.csv`.\n",
    "\n",
    "Hint: You can use the `pd.read_csv()` function to load in the data (remember to check the delimiter!). You can find the documentation [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load in the data\n",
    "\n",
    "# if you want to you can combine the two datasets into one - but this is not necessary\n",
    "\n",
    "# check a few rows of the data - hint: use .head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "### Check the number of missing values in the dataset.\n",
    "\n",
    "Hint: `.isnull()`\n",
    "\n",
    "Dont worry if there are missing values, we'll handle them later in our pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check some basic statistics\n",
    "\n",
    "We want to know the mean, standard deviation, minimum, maximum and quartiles of each feature.\n",
    "This will give us a good idea of the distribution of the data, and also tell us if we need to do any scaling.\n",
    "\n",
    "Hint: `.describe()`, If the output is hard to read, you can use `.T` to transpose the dataframe, i.e., swapping the rows and columns.\n",
    "\n",
    "Do you notice anything strange about the data? Is there anything that stands out to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some basic statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that there were some missing values in the dataset, this we can fix in the pipeline, using the [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) from sklearn.\n",
    "\n",
    "Next we also saw that there was a some differences in the scale of the different variables, so we will use the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) from sklearn to scale the data. This will make it easier for the model to learn the patterns in the data. Especially for the KNN algorithm (which we'll use), which is based on distance, it is important that the data is scaled.\n",
    "\n",
    "If you think of other transformations that might be useful for this dataset, feel free to try them out!\n",
    "\n",
    "**Take a look at the [sklearn.preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html) module for some inspiration.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data\n",
    "\n",
    "Now that we have created our pipeline, we can train the model.\n",
    "\n",
    "First we need to split the data into a training set and a test set. We will use the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from sklearn to do this. But first we need to split the data into features and labels.\n",
    "\n",
    "The features are all the columns in the dataset, except for the `quality` column, which are the labels.\n",
    "\n",
    "We will use the default split of 75% training data, and 25% test data.\n",
    "\n",
    "Hint: You can use the `random_state` parameter to make sure that the data is split the same way every time you run the code.\n",
    "\n",
    "The train_test_split function returns four values, the first two are the training and test data, and the last two are the train and test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split the data into X and y\n",
    "\n",
    "\n",
    "# now split the data into train and test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the pipeline\n",
    "\n",
    "We will now create a pipeline that will handle the missing values and scaling for us, and finally train a KNN model on the data.\n",
    "\n",
    "We will use the [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) class from sklearn to create our pipeline.\n",
    "\n",
    "The pipeline will consist of three steps, the first step will be to impute the missing values, and the second step will be to scale the data, and the third step will be to train the model.\n",
    "\n",
    "The pipeline format is a list of tuples, where the first element in the tuple is the name of the step, and the second element is the step itself, e.g.:\n",
    "\n",
    "```python\n",
    "pipeline = Pipeline([\n",
    "\t('step_name', step()),\n",
    "\t('step_name', step()),\n",
    "\t('step_name', step()),\n",
    "])\n",
    "```\n",
    "\n",
    "Where the `step_name` is a string, and the `step` is a sklearn object - this can be a \"Transformer\" object (like `SimpleImputer` and `StandardScaler`) or an \"Estimator\" object (like `KNeighborsClassifier` or `LinearRegression`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# create your pipeline\n",
    "pipe = Pipeline([\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model\n",
    "\n",
    "Now that we have trained the model, we want to evaluate it to see how well it performs.\n",
    "\n",
    "We will use the [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) function from sklearn to calculate the accuracy of the model.\n",
    "\n",
    "Since we have created a pipeline, we can simply call the `.fit()` and `.predict()` methods on the pipeline object, and it will handle the preprocessing for us - and importantly in the correct order.\n",
    "\n",
    "Remember to only call `.fit()` on the training data. Calling `.fit()` on the test data will cause the model to overfit to the test data, and will give you an overly optimistic accuracy score.\n",
    "\n",
    "* **`.fit(X_train, y_train)` will train the model on the training data.**\n",
    "* **`.predict(X_train)` will return the predicted labels for the test data, which you can then pass to the `accuracy_score` function, along with the true labels (y_train).**\n",
    "* **`.predict(X_test)` will return the predicted labels for the test data, which you can then pass to the `accuracy_score` function, along with the true labels (y_test).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# fit the pipeline\n",
    "\n",
    "\n",
    "# evaluate the pipeline\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
